{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import waterdetect as wd\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "\n",
    "import shapely.geometry\n",
    "from shapely.geometry import mapping\n",
    "from shapely.geometry import Point, LineString\n",
    "import shapely.geometry\n",
    "\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.morphology import skeletonize\n",
    "from skimage.graph import MCP_Geometric\n",
    "import cv2 as cv\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import collections\n",
    "import math\n",
    "\n",
    "import itertools\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### shared funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shared functions\n",
    "\n",
    "def create_new_dir(outdir):\n",
    "    ##Create output file\n",
    "    try:\n",
    "        os.mkdir(outdir)\n",
    "        print(f'{outdir} created to export results')\n",
    "    except:\n",
    "        pass \n",
    "\n",
    "def export_netcdf (outdir, target_da, filename):\n",
    "    #create output dir\n",
    "    out_dir = os.path.join(outdir, 'netCDF')\n",
    "    create_new_dir(out_dir)\n",
    "    #set out file path\n",
    "    out_file = os.path.join(out_dir, filename)\n",
    "    #convert datarray to dataset\n",
    "    DS = target_da.to_dataset(name='water')\n",
    "    #add crs to attrs\n",
    "    DS.water.attrs['crs'] = str(target_da.rio.crs)\n",
    "    #create dict for each variable for encoding\n",
    "    comp = dict(zlib=True, complevel=6)   ### need that much compression?\n",
    "    encoding = {var: comp for var in DS.data_vars}\n",
    "    #export compressed netCDF\n",
    "    DS.to_netcdf(out_file, encoding=encoding)                     \n",
    "    return out_file\n",
    "\n",
    "def estimate_time (total_iterations, t_begin, t_end, est_time_lst):\n",
    "    #calculate time to execute one iteration\n",
    "    est_time_lst.append(t_end-t_begin)\n",
    "    est_time_lst_av = sum(est_time_lst) / len(est_time_lst)  \n",
    "    total_est_seconds = total_iterations * est_time_lst_av\n",
    "    #convert total seconds to h,m,s\n",
    "    m, s = divmod(total_est_seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    if h != 0:\n",
    "        if h == 1:\n",
    "            print(f\"Estimated remaining time: {h:0.0f} hour, {m:0.0f} minutes and {s:0.0f} seconds\")\n",
    "        else:\n",
    "            print(f\"Estimated remaining time: {h:0.0f} hours, {m:0.0f} minutes and {s:0.0f} seconds\")\n",
    "    else:\n",
    "        if m == 1:\n",
    "            print(f\"Estimated remaining time: {m:0.0f} minute and {s:0.0f} seconds\")\n",
    "        elif m == 0:\n",
    "            print(f\"Estimated remaining time: {s:0.0f} seconds\")\n",
    "        else:\n",
    "            print(f\"Estimated remaining time: {m:0.0f} minutes and {s:0.0f} seconds\")\n",
    "    #update total_iterations\n",
    "    total_iterations -= 1\n",
    "    return total_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### wd_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaterDetectBatch():\n",
    "\n",
    "    def __init__(self, input_img, ini_file, rcor_extent, buffer = 1000, img_ext = '.tif', reg = None, max_cluster = None, export_tif = False):\n",
    "        self.input_img = input_img\n",
    "        self.ini_file = ini_file\n",
    "        self.rcor_extent = rcor_extent\n",
    "        self.buffer = buffer\n",
    "        self.img_ext = img_ext\n",
    "        self.reg = reg\n",
    "        self.max_cluster = max_cluster\n",
    "        self.export_tif = export_tif\n",
    "        self.outdir = None\n",
    "\n",
    "    def validate (self):\n",
    "        def _is_valid_input_img ():\n",
    "            print('Checking input data...')\n",
    "            #check if netCDF\n",
    "            if self.input_img.endswith('.nc') == True:\n",
    "                #open netCDF file as xarray\n",
    "                self.input_img = rxr.open_rasterio(self.input_img, decode_coords='all', chunks={'x': 1000, 'y': 1000})\n",
    "                crs = self.input_img.rio.crs\n",
    "                self.input_img = self.input_img.to_array(dim='band')\n",
    "                #fill outliers and set nodata values to 0\n",
    "                self.input_img = xr.where(((self.input_img > 0) & (self.input_img < 20000)), self.input_img, 0)\n",
    "                #for some reason ds lost crs after 'where'\n",
    "                self.input_img = self.input_img.rio.write_crs(crs)\n",
    "                #fill nodata and add to attrs\n",
    "                self.input_img = self.input_img.fillna(0)             \n",
    "                self.input_img.attrs['_FillValue'] = 0\n",
    "                # rename and transpose dims\n",
    "                # self.input_img = self.input_img.rename({'variable': 'band'})\n",
    "                self.input_img = self.input_img.transpose('time', 'band', 'y', 'x')\n",
    "                #check if arrays have at least VNIR bands\n",
    "                assert len(self.input_img.band) <= 4, 'Not enough data. Dataset must have at least 4 bands (B,G,R,NIR)'\n",
    "                #check if arrays have at least VNIR bands\n",
    "                if len(self.input_img.band) == 4:\n",
    "                    print('Reminder: 4 bands in source must be stacked as B,G,R,NIR')\n",
    "                else:\n",
    "                    print(f'{len(self.input_img.data_vars)} bands found. Reminder: First 6 bands must be stacked as B,G,R,NIR,SWIR1,SWIR2')\n",
    "                #check if arrays have time dimension\n",
    "                assert 'time' in self.input_img.dims, 'Dataset missing \"time\" dimension'\n",
    "                return self.input_img, len(self.input_img.band)\n",
    "            #check if folder  \n",
    "            elif self.input_img.endswith('.nc') == False and os.path.isdir(self.input_img) == True:\n",
    "                #create list of image paths\n",
    "                self.input_img = [os.path.join(self.input_img, f) for f in os.listdir(self.input_img) if f.endswith(self.img_ext)]\n",
    "                #check if date is valid and if crs and pixel size are the same\n",
    "                crs_lst = []\n",
    "                res_lst = []\n",
    "                band_lst = []\n",
    "                for img in self.input_img:\n",
    "                    pimg = rxr.open_rasterio(img)\n",
    "                    assert pd.Timestamp(os.path.basename(img)[3:13]), f'{img} Invalid Filename. Valid example: s2_2018-01-01.tif (sensor_id (2 letters) _ date (yyyy-mm-dd))'\n",
    "                    crs_lst.append(pimg.rio.crs)\n",
    "                    res_lst.append(pimg.rio.resolution()[0])\n",
    "                    band_lst.append(pimg.shape[0])\n",
    "                assert len(set(crs_lst)) == 1, f'Check projection. Images must have same EPSG'\n",
    "                assert len(set(res_lst)) == 1, f'Check spatial resolution. Images must have same pixel size'\n",
    "                assert len(set(band_lst)) == 1, f'Check spatial resolution. Images must have same number of bands'\n",
    "                #check if arrays have at least VNIR bands\n",
    "                if band_lst[0] == 4:\n",
    "                    print(f'Reminder: 4 bands in source must be stacked as B,G,R,NIR')\n",
    "                else:\n",
    "                    print(f'{band_lst[0]} bands found. Reminder: First 6 bands must be stacked as B,G,R,NIR,SWIR2')\n",
    "                #convert raster time series to dataarray\n",
    "                self.input_img = sorted(self.input_img)\n",
    "                filenames_time = pd.DatetimeIndex([pd.Timestamp(os.path.basename(f)[3:13]) for f in self.input_img])\n",
    "                time = xr.Variable('time', filenames_time)\n",
    "                self.input_img = xr.concat([rxr.open_rasterio(f, chunks={'x': 1000, 'y': 1000}) for f in self.input_img], dim=time)\n",
    "                self.input_img = self.input_img.fillna(0)             \n",
    "                self.input_img.attrs['_FillValue'] = 0\n",
    "                return self.input_img, band_lst[0]\n",
    "            #if provided input not netCDF or folder raise error\n",
    "            else:\n",
    "                raise ValueError('Pass input_img as a valid directory or netCDF file (.nc)')\n",
    "        #check input images or DataArray\n",
    "        self.input_img, n_bands = _is_valid_input_img ()        \n",
    "        #check if rcor_extent is a shapefile\n",
    "        assert self.rcor_extent.endswith('.shp'), 'Pass river corridor extent (rcor_extent) as .shp'\n",
    "        #check .ini extension\n",
    "        assert self.ini_file.endswith('.ini'), \"Use WaterDetect .ini file\"\n",
    "        print('Input data validated.')\n",
    "        return self.input_img, n_bands\n",
    "\n",
    "    def wd_batch(self):\n",
    "        def change_ini (self, n_bands):\n",
    "            #open ini file\n",
    "            a_file = open(self.ini_file, \"r\")\n",
    "            #read txt lines\n",
    "            list_of_lines = a_file.readlines()\n",
    "            #check number of bands and change ini file accordingly\n",
    "            if n_bands == 4:\n",
    "                bands = ['Blue', 'Green', 'Red', 'Nir']\n",
    "                if self.max_cluster == None and self.reg == None:\n",
    "                    self.max_cluster = 6\n",
    "                    self.reg = 0.07\n",
    "                list_of_lines[84] = \"#\\t\\t    ['mndwi', 'ndwi', 'Mir2'],\\n\"\n",
    "                list_of_lines[104] = \"\\t\\t    ['ndwi', 'Nir' ],\\n\"\n",
    "            else:\n",
    "                bands=['Blue', 'Green', 'Red', 'Nir', 'Mir', 'Mir2']\n",
    "                if self.max_cluster == None and self.reg == None:\n",
    "                    self.max_cluster = 3\n",
    "                    self.reg = 0.08\n",
    "                list_of_lines[84] = \"\\t\\t    ['mndwi', 'ndwi', 'Mir2'],\\n\"\n",
    "                list_of_lines[104] = \"#\\t\\t    ['ndwi', 'Nir' ],\\n\"\n",
    "            #set max_cluster and reg\n",
    "            list_of_lines[117] = 'regularization = ' + str(self.reg) + '\\n'\n",
    "            list_of_lines[124] = 'max_clusters = ' + str(self.max_cluster) + '\\n'\n",
    "            #open and edit default .ini file\n",
    "            a_file = open(self.ini_file, \"w\")\n",
    "            a_file.writelines(list_of_lines)\n",
    "            a_file.close()\n",
    "            return self.ini_file, bands\n",
    "\n",
    "        def buffer_clip_aoi (self):\n",
    "            #read rcor_extent as geodataframe\n",
    "            gpd_rcor_extent = gpd.read_file(self.rcor_extent)\n",
    "            #create a buffer using buffer value\n",
    "            rcor_extent_buffer = gpd_rcor_extent.buffer(self.buffer)\n",
    "            #reproject aoi if needed\n",
    "            if rcor_extent_buffer.crs.to_epsg() != self.input_img.rio.crs.to_epsg():\n",
    "                rcor_extent_buffer = rcor_extent_buffer.to_crs(self.input_img.rio.crs.to_epsg())\n",
    "            #clip raster to aoi\n",
    "            self.input_img = self.input_img.rio.clip(rcor_extent_buffer.geometry.apply(mapping), rcor_extent_buffer.crs)\n",
    "            return self.input_img\n",
    "\n",
    "        def create_wd_dict (self, img_target, bands):\n",
    "            #rescale bands\n",
    "            div_factor = 10000\n",
    "            cube = img_target/div_factor\n",
    "            arrays = {}   \n",
    "            #create dict for wd input\n",
    "            for i, layer in enumerate(bands):\n",
    "                arrays[layer] = cube[i].values\n",
    "            #create dummy DataArray\n",
    "            water_xarray = img_target.isel(band=1)\n",
    "            return arrays, water_xarray\n",
    "\n",
    "        def wd_mask (self, img_target, water_xarray):\n",
    "            #create_wd_dict from data arrays\n",
    "            arrays, water_xarray = create_wd_dict(self, img_target, bands)\n",
    "            #run wd depending on the number of bands\n",
    "            if len(bands) == 4:\n",
    "                # As we don't have Mir band, it will cheat the water detect (to be corrected in the WaterDetect next version)\n",
    "                arrays['mndwi'] = np.zeros_like(arrays['Green'])\n",
    "                arrays['mbwi'] = np.zeros_like(arrays['Green'])\n",
    "                arrays['Mir2'] = np.zeros_like(arrays['Green'])\n",
    "                invalid_mask = (arrays['Nir'] == 0)\n",
    "                #run wd\n",
    "                wmask = wd.DWImageClustering(bands=arrays, bands_keys=['ndwi', 'Nir'], invalid_mask=invalid_mask, config=config, glint_processor=None)\n",
    "                mask = wmask.run_detect_water()\n",
    "                water_xarray.values= wmask.water_mask\n",
    "                water_xarray.rio.write_nodata(-1, inplace=True)\n",
    "                clear_output()\n",
    "                return water_xarray\n",
    "            else:\n",
    "                invalid_mask = (arrays['Mir2'] == 0)\n",
    "                #run wd\n",
    "                wmask = wd.DWImageClustering(bands=arrays, bands_keys=['mndwi', 'ndwi', 'Mir2'], invalid_mask=invalid_mask, config=config, glint_processor=None)\n",
    "                mask = wmask.run_detect_water()\n",
    "                water_xarray.values= wmask.water_mask\n",
    "                water_xarray.rio.write_nodata(-1, inplace=True)\n",
    "                clear_output()\n",
    "                return water_xarray\n",
    "\n",
    "        #execute class\n",
    "        import time\n",
    "        #create output dir\n",
    "        self.outdir = os.path.join(os.path.dirname(os.path.abspath(self.ini_file)), 'results_RiverConnect')\n",
    "        outdir = self.outdir\n",
    "        create_new_dir(self.outdir)\n",
    "        self.outdir = os.path.join(self.outdir, 'wd_batch')\n",
    "        create_new_dir(self.outdir)\n",
    "        #create dir to export tif files\n",
    "        if self.export_tif == True:\n",
    "            tif_out_dir = os.path.join(self.outdir, 'wmask_tif')\n",
    "            create_new_dir(tif_out_dir)\n",
    "        #validate inputs\n",
    "        self.input_img, n_bands = self.validate ()\n",
    "        #edit ini file\n",
    "        self.ini_file, bands = change_ini (self, n_bands)\n",
    "        #configure wd\n",
    "        config = wd.DWConfig(config_file= self.ini_file)\n",
    "        config.detect_water_cluster\n",
    "        #image buffer aoi and clip\n",
    "        self.input_img = buffer_clip_aoi (self)\n",
    "        #create lists to store results\n",
    "        wd_lst = []\n",
    "        time_lst = []\n",
    "        est_time_lst = []\n",
    "        total_iterations = len(self.input_img)\n",
    "        #iterate through timesteps\n",
    "        for img_target in self.input_img:\n",
    "            t_begin = time.perf_counter()\n",
    "            #retrieve date\n",
    "            date = pd.to_datetime(str(img_target.time.values)).strftime('%Y-%m-%d')\n",
    "            print(f'Executing {date}')\n",
    "            #append date to list\n",
    "            time_lst.append(date)\n",
    "            #execute wd\n",
    "            water_xarray = wd_mask (self, img_target, bands)\n",
    "            #append results\n",
    "            wd_lst.append(water_xarray)\n",
    "            #export rasters\n",
    "            if self.export_tif == True:\n",
    "                water_xarray.rio.to_raster(os.path.join(tif_out_dir, str(date) + '.tif'), compress='lzw')\n",
    "            t_end = time.perf_counter()\n",
    "            #estimate time\n",
    "            total_iterations = estimate_time (total_iterations, t_begin, t_end, est_time_lst)\n",
    "        clear_output()\n",
    "        print('Working on results...')\n",
    "        #create a time DataArray\n",
    "        time = xr.Variable('time', pd.DatetimeIndex(time_lst))\n",
    "        #concatenate results\n",
    "        da_wmask = xr.concat(wd_lst, dim=time).chunk(chunks= {'x': 1000, 'y': 1000})\n",
    "        #make sure results have crs\n",
    "        da_wmask.rio.write_crs(self.input_img.rio.crs, inplace=True)\n",
    "        #reproject results to UTM WGS84\n",
    "        da_wmask = da_wmask.rio.reproject(da_wmask.rio.estimate_utm_crs(), resolution = round(self.input_img.rio.resolution()[0],2)).chunk(chunks= {'x': 1000, 'y': 1000})\n",
    "        clear_output()\n",
    "        #export results to netcdf\n",
    "        print('Exporting results...')\n",
    "        export_netcdf (self.outdir, da_wmask, 'da_wmask.nc')\n",
    "        clear_output()\n",
    "        print('All done!')\n",
    "\n",
    "        return da_wmask, outdir\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics + Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    \n",
    "    def __init__(self, da_wmask, rcor_extent, export_shp = False, export_raster = True, outdir=None, sec_length= None):\n",
    "        \n",
    "        self.da_wmask = da_wmask\n",
    "        self.rcor_extent = rcor_extent\n",
    "        self.export_shp = export_shp\n",
    "        self.outdir = outdir\n",
    "        \n",
    "        self.sec_length = sec_length\n",
    "        \n",
    "        self.export_raster = export_raster\n",
    "        \n",
    "    def validate (self):\n",
    "        \n",
    "        print('Checking input data...')\n",
    "        \n",
    "        assert type(self.da_wmask) == str or type(self.da_wmask) == xr.core.dataarray.DataArray, 'Invalid Input. Pass da_wmask as a valid netCDF file (.nc) or DataArray'\n",
    "        \n",
    "        if type(self.da_wmask) == str:\n",
    "            assert self.da_wmask.endswith('.nc'), 'Invalid Input. Pass da_wmask as a valid netCDF file (.nc)'\n",
    "            \n",
    "            if os.path.basename(os.path.dirname(os.path.abspath(da_wmask))) == 'netCDF':\n",
    "                self.outdir = os.path.join(os.path.dirname((os.path.dirname(os.path.dirname(os.path.abspath(self.da_wmask))))))\n",
    "            else:\n",
    "                self.outdir = os.path.dirname(os.path.abspath(self.da_wmask))\n",
    "                \n",
    "            self.da_wmask = rxr.open_rasterio(self.da_wmask, decode_coords='all')\n",
    "            crs = self.da_wmask.rio.crs\n",
    "            self.resolution = self.da_wmask.rio.resolution()\n",
    "            \n",
    "            if type(self.da_wmask) == xr.core.dataset.Dataset:\n",
    "                self.da_wmask = self.da_wmask.to_array(dim='band')\n",
    "                self.da_wmask  = self.da_wmask.squeeze(dim='band')\n",
    "                self.da_wmask = self.da_wmask.rio.write_crs(crs)\n",
    "                \n",
    "            self.da_wmask.attrs['_FillValue'] = -1\n",
    "            \n",
    "            assert self.da_wmask.rio.crs != None, 'CRS not found.'\n",
    "            assert self.da_wmask.dims == ('time', 'y', 'x'), \"Dimensions not ('time', 'y', 'x')\"\n",
    "        \n",
    "        else: \n",
    "            \n",
    "            assert self.da_wmask.rio.crs != None, 'CRS not found.'\n",
    "            assert self.da_wmask.dims == ('time', 'y', 'x'), \"Dimensions not ('time', 'y', 'x')\"\n",
    "        \n",
    "        #check rcor_extent extension\n",
    "        assert self.rcor_extent.endswith('.shp'), 'Pass river corridor extent (rcor_extent) as .shp'\n",
    "        \n",
    "        clear_output()\n",
    "        return self.da_wmask\n",
    "    \n",
    "    def fill_nodata (self, bbox_da_mask):\n",
    "        #fill no data with condition\n",
    "            #for each time layer check if no data exists, if so, fill data with next layer pixel value \n",
    "        print('Filling NoData...')\n",
    "        \n",
    "        # for num, array in enumerate(bbox_da_mask):\n",
    "        #     try:\n",
    "        #         bbox_da_mask[num]= xr.where(bbox_da_mask[num].isin(-1) , bbox_da_mask[num+1], bbox_da_mask[num])\n",
    "        #     except:\n",
    "        #         try:\n",
    "        #             bbox_da_mask[num]= xr.where(bbox_da_mask[num].isin(-1) , bbox_da_mask[num-1], bbox_da_mask[num])\n",
    "        #         except:    \n",
    "        #             try:\n",
    "        #                 bbox_da_mask[num]= xr.where(bbox_da_mask[num].isin(-1) , bbox_da_mask[num+2], bbox_da_mask[num])\n",
    "        #             except:\n",
    "        #                 try:\n",
    "        #                     bbox_da_mask[num]= xr.where(bbox_da_mask[num].isin(-1) , bbox_da_mask[num-2], bbox_da_mask[num])\n",
    "        #                 except:\n",
    "        #                     pass\n",
    "        \n",
    "        for num, array in enumerate(bbox_da_mask):\n",
    "            try:\n",
    "                bbox_da_mask[num]= xr.where(bbox_da_mask[num].isin(-1) , bbox_da_mask[num+1], bbox_da_mask[num])\n",
    "            except:\n",
    "                bbox_da_mask[num]= xr.where(bbox_da_mask[num].isin(-1) , bbox_da_mask[num-1], bbox_da_mask[num])\n",
    "            try:\n",
    "                bbox_da_mask[num]= xr.where(bbox_da_mask[num].isin(-1) , bbox_da_mask[num+2], bbox_da_mask[num])\n",
    "            except:\n",
    "                try:\n",
    "                    bbox_da_mask[num]= xr.where(bbox_da_mask[num].isin(-1) , bbox_da_mask[num-2], bbox_da_mask[num])\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        \n",
    "        clear_output()\n",
    "        return bbox_da_mask#.chunk(chunks= {'x': 1000, 'y': 1000})\n",
    "    \n",
    "    #clip and calculate wetted area\n",
    "    def input_process (self, rcor_extent, layer):\n",
    "        \n",
    "        # reproject xarray to UTM wgs84\n",
    "        # layer = layer.rio.reproject(layer.rio.estimate_utm_crs(), resolution = 10)#, resolution = layer.rio.resolution()[0]) ##############\n",
    "        layer = layer.rio.reproject(layer.rio.estimate_utm_crs(), resolution = self.resolution[0])\n",
    "        self.crs_wgs = layer.rio.crs\n",
    "        #reproject aoi if needed\n",
    "        if rcor_extent.crs.to_epsg() != layer.rio.crs.to_epsg():\n",
    "            rcor_extent= rcor_extent.to_crs(layer.rio.crs.to_epsg())\n",
    "        #clip raster to aoi\n",
    "        _layer = layer.rio.clip(rcor_extent.geometry, rcor_extent.crs)\n",
    "        \n",
    "        #copy _layer to return clipped array\n",
    "        layer_clip = _layer.values.copy()\n",
    "        #change any other values than 1 to zero\n",
    "        _layer= xr.where(_layer== 1, _layer, 0)\n",
    "        \n",
    "        #Calculate euclidean distance transform array\n",
    "        euc_dist_trans_array = ndimage.distance_transform_edt(_layer)\n",
    "        \n",
    "        #group connected pixels into groups\n",
    "        pre_label= label(_layer, connectivity=2)\n",
    "        pre_label= np.where(_layer == 1, pre_label, 0)\n",
    "        \n",
    "        _layer = _layer.rio.write_crs(self.crs_wgs)\n",
    "        _layer.attrs['_FillValue'] = 0\n",
    "        \n",
    "        # #get each group properties\n",
    "        pre_label_shapes = list(rasterio.features.shapes((pre_label.astype('int32')), transform=_layer.rio.transform()))\n",
    "        data = [(value, shapely.geometry.shape(polygon).length, shapely.geometry.shape(polygon).area) for polygon, value in pre_label_shapes]\n",
    "\n",
    "        d = defaultdict(list)\n",
    "        for key, group in itertools.groupby(sorted(data), operator.itemgetter(0)):    \n",
    "            len_sum = sum([float(t[1]) for t in group])\n",
    "            d[key].append(len_sum)\n",
    "        for key, group in itertools.groupby(sorted(data), operator.itemgetter(0)):  \n",
    "            area_sum = sum([float(t[2]) for t in group])\n",
    "            d[key].append(area_sum)\n",
    "\n",
    "        d_area = {key: val[0] for key, val in d.items()}\n",
    "        u,inv = np.unique(pre_label, return_inverse = True)\n",
    "        area_array = np.array([d_area[x] for x in u])[inv].reshape(pre_label.shape)\n",
    "        \n",
    "        #total wet area\n",
    "        total_wet_area= [(sum((x[1]) for key, x in d.items() if key != 0))]\n",
    "        #total perimeter\n",
    "        total_wet_perimeter = [(sum((x[0]) for key, x in d.items() if key != 0))]\n",
    "        # AWMSI\n",
    "        AWMSI = [(sum(((0.25*x[0]/np.sqrt(x[1]))*((x[1])/total_wet_area[0])) for key, x in d.items() if key != 0))]\n",
    "        \n",
    "        # AWMPS\n",
    "        try:\n",
    "            AWMPS = np.average([x[1] for key, x in d.items() if key != 0], weights = [x[1] for key, x in d.items() if key != 0])\n",
    "        except:\n",
    "            AWMPS = 0       \n",
    "\n",
    "        #skeletonize water mask\n",
    "        _layer.values = skeletonize(_layer, method='lee')\n",
    "        _layer= _layer.chunk(chunks= {'x': 1000, 'y': 1000})\n",
    "        layer_skel = _layer.copy()\n",
    "        #change any other values than 1 to zero\n",
    "        _layer= xr.where(_layer== 0, _layer, 1)\n",
    "        # find endpoints by checking neighbourhood\n",
    "        kernel = np.uint8([[1,  1, 1],\n",
    "                            [1, 10, 1],\n",
    "                            [1,  1, 1]])\n",
    "        #calculate kernel\n",
    "        end_point_filter= cv.filter2D(_layer.values, -1, kernel)\n",
    "        #retrieve 11 and 13 values - representing potential endpoints\n",
    "        end_points= np.argwhere((end_point_filter==11) | (end_point_filter==13))\n",
    "        #create lists to store results and check duplicates\n",
    "        _dict_dist= {}\n",
    "        dup_list= []\n",
    "        #group connected pixels into groups\n",
    "        _layer.values= label(_layer, connectivity=2)\n",
    "        _layer= _layer.chunk(chunks= {'x': 1000, 'y': 1000})\n",
    "        #iterate through end points\n",
    "        for i in end_points:\n",
    "            #check item index\n",
    "            end_label= _layer[i[0], i[1]]\n",
    "            #retrieve label\n",
    "            end_label= end_label.values.item()\n",
    "            dup_list.append(end_label)\n",
    "            #check for dups\n",
    "            count= dup_list.count(end_label)\n",
    "            #if no dups found create new item, else append to existing item\n",
    "            if count == 1:\n",
    "                _dict_dist[end_label]= {1: {'id': 1, 'np': [i[0], i[1]]}}\n",
    "            else:\n",
    "                _dict_dist[end_label][count]= {'id': count, 'np': [i[0], i[1]]}\n",
    "        _layer= xr.where(_layer!= 0, 1, -1)\n",
    "        _layer.values= _layer.values.astype('int8')\n",
    "        _layer= _layer.chunk(chunks= {'x': 1000, 'y': 1000})\n",
    "        return layer_clip, total_wet_area, total_wet_perimeter, AWMSI, layer_skel, end_points, _layer, _dict_dist, area_array, euc_dist_trans_array, AWMPS\n",
    "    \n",
    "    def filter_max_dist(self, _dict_dist):\n",
    "        #create lists to store results\n",
    "        filtered_max_list= []\n",
    "        #iterate through dicts (each label)\n",
    "        for _dict in _dict_dist:\n",
    "            #create list to store partial results\n",
    "            max_dist_list=[]\n",
    "            #calculate all possible combinations for end points\n",
    "            comb_list= list(combinations(_dict_dist[_dict].keys(), 2))\n",
    "            #calculate euclidean dist for each combination\n",
    "            for item in comb_list:\n",
    "                x= np.array(_dict_dist[_dict][item[0]]['np'])\n",
    "                y= np.array(_dict_dist[_dict][item[1]]['np'])\n",
    "                dist = np.linalg.norm(x - y)\n",
    "                _dict_comb_filter= {'keys': item, 'dist': dist, 'start': x, 'end': y}\n",
    "                max_dist_list.append(_dict_comb_filter)\n",
    "            #sort calculated distance\n",
    "            max_dist_list.sort(key=lambda x: x['dist'], reverse=True)\n",
    "            #select greatest five distances and append to results list\n",
    "            max_dist_list= max_dist_list[0:5]\n",
    "            filtered_max_list.append(max_dist_list)\n",
    "        #create list to store partial results    \n",
    "        small_list= []\n",
    "        #iterate through results list and filter values bigger than 2 pixels\n",
    "        for num, group in enumerate(filtered_max_list):\n",
    "            if ((sum((item['dist']) for item in group))) <= 2:\n",
    "                small_list.append(num)\n",
    "        filtered_max_list = [v for i, v in enumerate(filtered_max_list) if i not in small_list]\n",
    "        #export results as a dict\n",
    "        filtered_max_dist={}\n",
    "        for num, value in enumerate(filtered_max_list):\n",
    "            filtered_max_dist[num]= value\n",
    "        return filtered_max_dist\n",
    "\n",
    "    def calculate_connectivity_properties(self, _layer, filtered_max_dist):\n",
    "        def least_cost(m, start, end):\n",
    "            #calculate minimum cost path\n",
    "            costs, traceback_array = m.find_costs([start], [end])\n",
    "            return m.traceback(end), costs[end]\n",
    "        def np_position_to_coord_point_list (np_position, _layer):\n",
    "            #create list to store coord results\n",
    "            coord_list= []\n",
    "            for i in np_position:\n",
    "                #retrieve x\n",
    "                x= _layer['x'][i[1]]\n",
    "                x= np.reshape(x.values, (1,1))\n",
    "                x= x[0][0].astype('float32')\n",
    "                #retrieve y\n",
    "                y= _layer['y'][i[0]]\n",
    "                y= np.reshape(y.values, (1,1))\n",
    "                y= y[0][0].astype('float32')\n",
    "                #append x,y as shapely objects\n",
    "                coord_list.append(Point(x, y))\n",
    "            return coord_list\n",
    "        #create dict to store main results\n",
    "        _dict_wet_prop={}\n",
    "        #initialize skimage minimum cost path \n",
    "        m = MCP_Geometric(_layer, fully_connected=True)\n",
    "        #create list to store results\n",
    "        n_pools_index_lst = []\n",
    "        lines_index_lst = []\n",
    "        lines_index_lst_width = []\n",
    "        length_area_index_lst = []\n",
    "        #iterate through filtered_max_dist\n",
    "        for idf in filtered_max_dist:\n",
    "            #create dict to store partial results\n",
    "            _dict_comb={}\n",
    "            #calculate minimum cost path for each label\n",
    "            for num, item in enumerate(filtered_max_dist[idf], start=1):\n",
    "                cost_path, dist= least_cost(m, tuple(item['start']), tuple(item['end']))\n",
    "                _dict_comb[num]= {'dist': dist, 'path': cost_path}\n",
    "            try:\n",
    "                #retrieve max distance for label\n",
    "                max_dist= max((d['dist']) for d in _dict_comb.values())\n",
    "                #retrieve results for max dist label\n",
    "                for d in _dict_comb.values():\n",
    "                    if d['dist']== max_dist:\n",
    "                        #retrieve results for exporting xarray\n",
    "                        n_pools_index_lst.append(d['path'][0])\n",
    "                        n_pools_index_lst.append(d['path'][-1])\n",
    "                        lines_index_lst.extend(d['path'])\n",
    "                        \n",
    "                        #retrieve results for exporting shp\n",
    "                        coord_list= np_position_to_coord_point_list (d['path'], _layer)\n",
    "                        line_path = LineString(coord_list)\n",
    "                        _dict_wet_prop[idf]= {'coord_start': Point(line_path.coords[0]),\n",
    "                                              'coord_end': Point(line_path.coords[-1]),\n",
    "                                              'length': line_path.length, \n",
    "                                              'linestring': line_path}\n",
    "                        \n",
    "                        length_area_index_lst.append((d['path'][0], line_path.length))\n",
    "                        lines_index_lst_width.append((d['path'], line_path.length))\n",
    "                    else:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "        return _dict_wet_prop, n_pools_index_lst, lines_index_lst, length_area_index_lst, lines_index_lst_width\n",
    "    \n",
    "    def save_results_xr (self, layer_clip, index_lst):\n",
    "        #create dummy array with nodata and zeros\n",
    "        dummy = layer_clip#.load()\n",
    "        dummy.fillna(-1)\n",
    "        dummy = xr.where(dummy == -1, dummy, 0)\n",
    "        #retrieve index from valid points\n",
    "        ind_x = []\n",
    "        ind_y = []\n",
    "        for indx in index_lst:\n",
    "            ind_x.append(indx[0])\n",
    "            ind_y.append(indx[1])\n",
    "        #create xarray for idx and idy\n",
    "        dind_x = xr.DataArray(ind_x, dims=['points'])\n",
    "        dind_y = xr.DataArray(ind_y, dims=['points'])\n",
    "        try:\n",
    "            #populate valid indices with 1\n",
    "            dummy[dind_x, dind_y] = 1\n",
    "        except:\n",
    "            print('ERROR: Check Layer')\n",
    "        return dummy\n",
    "\n",
    "    def save_shp (self, _dict_wet_prop):\n",
    "        \n",
    "        out_shp_dir = os.path.join(self.outdir, 'shp')\n",
    "        create_new_dir (out_shp_dir)\n",
    "        \n",
    "        n_pools_gdf = gpd.GeoDataFrame()\n",
    "        d_line_gdf = gpd.GeoDataFrame()\n",
    "        \n",
    "        for d in _dict_wet_prop:\n",
    "            start = {'date': self.date, 'length': _dict_wet_prop[d]['length'],  'geometry': _dict_wet_prop[d]['coord_start']}\n",
    "            n_pools_gdf= n_pools_gdf.append(start, ignore_index=True)\n",
    "            end = {'date': self.date, 'length': _dict_wet_prop[d]['length'], 'geometry': _dict_wet_prop[d]['coord_end']}\n",
    "            n_pools_gdf= n_pools_gdf.append(end, ignore_index=True)\n",
    "\n",
    "            line = {'date': self.date, 'length': _dict_wet_prop[d]['length'], 'geometry': _dict_wet_prop[d]['linestring']}\n",
    "            d_line_gdf= d_line_gdf.append(line, ignore_index=True)\n",
    "\n",
    "        n_pools_gdf= n_pools_gdf.set_crs(self.crs_wgs)\n",
    "        n_pools_gdf.to_file(os.path.join(out_shp_dir, 'dpoints_' + self.date + '.shp'))\n",
    "\n",
    "        d_line_gdf= d_line_gdf.set_crs(self.crs_wgs)\n",
    "        d_line_gdf.to_file(os.path.join(out_shp_dir, 'rline_' + self.date + '.shp'))  \n",
    "        \n",
    "    \n",
    "    #### CALC METRICS #########\n",
    "\n",
    "    # def calc_metrics (self):\n",
    "    def ini_metrics (self):\n",
    "\n",
    "        pdm = self.pd_metrics[['wet_area_km2', 'wet_perimeter_km2', 'wet_length_km', 'sec_area', 'npools', 'AWMSI', 'AWRe', \n",
    "                               'AWMPS', 'AWMPL', 'AWMPW', 'MPW']].copy()\n",
    "\n",
    "        pdm['sec_length'] = self.sec_length\n",
    "\n",
    "        pdm.loc[:,'APSEC'] = ((pdm['wet_area_km2']/pdm['sec_area'])*100).replace(np.inf, 0.0)\n",
    "        pdm.loc[:,'LPSEC'] = ((pdm['wet_length_km']/self.sec_length)*100).replace(np.inf, 0.0)\n",
    "\n",
    "        pdm.loc[:,'PDS'] = (pdm['npools']/pdm['wet_area_km2']).replace(np.inf, 0.0)\n",
    "        pdm.loc[:,'PDT'] = (pdm['npools']/pdm['wet_length_km']).replace(np.inf, 0.0)       \n",
    "\n",
    "        pdm.loc[:, :] = pdm.replace(np.nan, 0)\n",
    "        pdm.loc[:, :] = pdm.replace(np.inf, 0)           \n",
    "\n",
    "        pdm['date'] = pd.to_datetime(self.pd_metrics['date'], format='%Y-%m-%d')\n",
    "\n",
    "        col_order = ['date', 'sec_area', 'sec_length', 'wet_area_km2', 'wet_perimeter_km2', 'wet_length_km', 'npools', \n",
    "                      'AWMSI', 'AWRe', 'AWMPS', 'AWMPL', 'AWMPW', 'MPW', 'APSEC', 'LPSEC', 'PDS', 'PDT']\n",
    "\n",
    "        pdm = pdm[col_order]\n",
    "\n",
    "        return pdm \n",
    "    \n",
    "    \n",
    "    def pixel_level (self, da_area, pdm):\n",
    "\n",
    "        def export_raster(self, data, crs, fill, filename):\n",
    "            #set dataarray nodata\n",
    "            data.attrs['_FillValue'] = fill\n",
    "            #set crs\n",
    "            data = data.rio.write_crs(crs)\n",
    "            #export raster\n",
    "            data.rio.to_raster(os.path.join(self.outdir_raster, filename), compress='lzw', lock = True)\n",
    "\n",
    "        def persist_area (self, da_area, pdm):\n",
    "            \n",
    "            # if type(self.da_area) == str:\n",
    "            #     # self.da_area = rxr.open_rasterio(self.da_area, decode_coords='all')\n",
    "            #     self.da_area = xr.open_dataset(self.da_area, decode_coords='all')               \n",
    "            # #save crs\n",
    "            \n",
    "            self.crs = da_area.rio.crs\n",
    "            \n",
    "            #count timesteps\n",
    "            total_obs = int(da_area['time'].count().values)\n",
    "            #calculate pixel persistence\n",
    "            p_area = (da_area.sum(dim='time')/total_obs).astype('float32')\n",
    "            #change values less than zero to nodata\n",
    "            p_area = xr.where(p_area >= 0, p_area, -1)\n",
    "            #set crs\n",
    "            p_area = p_area.rio.write_crs(self.crs)\n",
    "            \n",
    "            pdm['PP'] = np.nanmean(xr.where(p_area >= 0.25, p_area, np.nan).values)\n",
    "            pdm['RA'] = (((np.nansum(xr.where(p_area >= 0.9, 1, np.nan).values))*self.resolution[0]*self.resolution[0])/10**6)\n",
    "            #Group 1 only >\n",
    "            pdm['PP_m25'] = (((np.nansum(xr.where(p_area >= 0.25, 1, np.nan).values))*self.resolution[0]*self.resolution[0])/10**6)\n",
    "            pdm['PP_m45'] = (((np.nansum(xr.where(p_area >= 0.45, 1, np.nan).values))*self.resolution[0]*self.resolution[0])/10**6)\n",
    "            pdm['PP_m50'] = (((np.nansum(xr.where(p_area >= 0.5, 1, np.nan).values))*self.resolution[0]*self.resolution[0])/10**6)\n",
    "            pdm['PP_m65'] = (((np.nansum(xr.where(p_area >= 0.65, 1, np.nan).values))*self.resolution[0]*self.resolution[0])/10**6)\n",
    "            pdm['PP_m75'] = (((np.nansum(xr.where(p_area >= 0.75, 1, np.nan).values))*self.resolution[0]*self.resolution[0])/10**6)\n",
    "            \n",
    "            #Group 2 only intervals            \n",
    "            pdm['PP_i25_50'] = (((np.nansum(xr.where((p_area >= 0.25) & (p_area <= 0.5), 1, np.nan).values))*self.resolution[0]*self.resolution[0])/10**6)\n",
    "            pdm['PP_i50_75'] = (((np.nansum(xr.where((p_area >= 0.5) & (p_area <= 0.75), 1, np.nan).values))*self.resolution[0]*self.resolution[0])/10**6)\n",
    "            pdm['PP_i75_90'] = (((np.nansum(xr.where((p_area >= 0.75) & (p_area <= 0.9), 1, np.nan).values))*self.resolution[0]*self.resolution[0])/10**6)\n",
    "            \n",
    "            #Group 3 only intervals            \n",
    "            pdm['PP_i25_45'] = (((np.nansum(xr.where((p_area >= 0.25) & (p_area <= 0.45), 1, np.nan).values))*self.resolution[0]*self.resolution[0])/10**6)\n",
    "            pdm['PP_i45_65'] = (((np.nansum(xr.where((p_area >= 0.45) & (p_area <= 0.65), 1, np.nan).values))*self.resolution[0]*self.resolution[0])/10**6)\n",
    "            pdm['PP_i65_90'] = (((np.nansum(xr.where((p_area >= 0.65) & (p_area <= 0.9), 1, np.nan).values))*self.resolution[0]*self.resolution[0])/10**6)\n",
    "\n",
    "            # pdm.loc['PP'] = np.nanmean(xr.where(p_area >= 0.25, p_area, np.nan).water.values)\n",
    "            # pdm.loc['P90_km2'] = (((np.nansum(xr.where(p_area >= 0.9, 1, np.nan).water.values))*100)/10**6)\n",
    "            \n",
    "            if self.export_raster == True:           \n",
    "                # export water persistence\n",
    "                export_raster(self, p_area, self.crs, -1, 'pwater_area.tif')\n",
    "\n",
    "            return pdm\n",
    "        \n",
    "        return persist_area (self, da_area, pdm)\n",
    "        \n",
    "    \n",
    "    def run(self):\n",
    "        import time\n",
    "        \n",
    "        def create_dataarray (_layer, layer_metric, date_list):\n",
    "            da = xr.DataArray(data = layer_metric,\n",
    "                              dims= ['time', 'y', 'x'],\n",
    "                              coords=dict(\n",
    "                                            time = date_list,\n",
    "                                            y=_layer.coords['y'],\n",
    "                                            x=_layer.coords['x'],\n",
    "                                )\n",
    "                            )\n",
    "            da.rio.write_crs(self.crs_wgs, inplace=True)\n",
    "            da.attrs['crs'] = str(self.crs_wgs)\n",
    "            da.attrs['_FillValue'] = -1\n",
    "            return da\n",
    "        \n",
    "\n",
    "            \n",
    "        #execute metrics        \n",
    "        self.da_wmask = self.validate()\n",
    "        \n",
    "        self.outdir = os.path.join(self.outdir, 'metrics')\n",
    "        first_path = self.outdir\n",
    "        create_new_dir(self.outdir)\n",
    "        print('Results from Metrics module will be exported to ', self.outdir)\n",
    "        \n",
    "        #read rcor_extent .shp as a gdf\n",
    "        rcor_extent = gpd.read_file(self.rcor_extent)\n",
    "        for polygon in rcor_extent.iloc:\n",
    "            \n",
    "            da_area = 0\n",
    "            da_npools = 0\n",
    "            da_rlines = 0\n",
    "            \n",
    "            seg_area = polygon.geometry.area/10**6\n",
    "            \n",
    "            aoi_poly = gpd.GeoDataFrame({'geometry': [polygon.geometry]}, crs=rcor_extent.crs)\n",
    "            bbox = aoi_poly.to_crs(self.da_wmask.rio.crs.to_epsg())\n",
    "            bbox_da_mask = self.da_wmask.rio.clip_box(bbox.bounds.iloc[0][0], bbox.bounds.iloc[0][1], \n",
    "                                                      bbox.bounds.iloc[0][2], bbox.bounds.iloc[0][3], auto_expand=True)\n",
    "            \n",
    "            bbox_da_mask = self.fill_nodata(bbox_da_mask)\n",
    "            \n",
    "            #create lists to store results\n",
    "            date_list= []\n",
    "            length_list= []\n",
    "            n_pools_list= []\n",
    "            total_wet_area_list= []\n",
    "            total_wet_perimeter_list = []\n",
    "            \n",
    "            AWMSI_arr = np.empty([0])\n",
    "            AWRe_arr = np.empty([0])\n",
    "            AWMPS_arr = np.empty([0])\n",
    "            AWMPL_arr = np.empty([0])\n",
    "            AWMPW_arr = np.empty([0])\n",
    "            \n",
    "            MPW_arr = np.empty([0])\n",
    "            \n",
    "            area_np_lst = []\n",
    "            length_np_lst = []\n",
    "            d_point_np_lst = []\n",
    "            \n",
    "            if len(rcor_extent.index) > 1:\n",
    "                self.outdir = os.path.join(first_path, str(polygon.name))\n",
    "                create_new_dir(self.outdir)\n",
    "                print('Results from Metrics module will be exported to ', self.outdir)\n",
    "            \n",
    "            est_time_lst = []\n",
    "            total_iterations = len(self.da_wmask)\n",
    "            #execute metrics\n",
    "\n",
    "            for num, layer in enumerate(bbox_da_mask): #(self.da_wmask):\n",
    "\n",
    "                print('Calculating metrics...')\n",
    "                self.date = pd.to_datetime(str(self.da_wmask.time.values[num])).strftime('%Y-%m-%d')\n",
    "                print(f'Executing {self.date}')\n",
    "\n",
    "                #time\n",
    "                t_begin = time.perf_counter()\n",
    "                \n",
    "                layer_clip, total_wet_area, total_wet_perimeter, AWMSI, layer_skel,\\\n",
    "                end_points, _layer, _dict_dist, area_array, euc_dist_trans_array, AWMPS = self.input_process (aoi_poly, layer)\n",
    "                \n",
    "                print('input_process ready')\n",
    "                filtered_max_dist = self.filter_max_dist(_dict_dist)\n",
    "                print('filtered_max_dist ready...')\n",
    "                _dict_wet_prop, n_pools_index_lst, lines_index_lst, length_area_index_lst, lines_index_lst_width = self.calculate_connectivity_properties(_layer, filtered_max_dist)\n",
    "                print('calculate_connectivity_properties ready...')\n",
    "                \n",
    "                #Calculate Elongation for each pool\n",
    "                idx, idy = [x[0][0] for x in length_area_index_lst],  [y[0][1] for y in length_area_index_lst]\n",
    "                length = [l[1] for l in length_area_index_lst]            \n",
    "                Re_param = list(zip(area_array[idx, idy], length))\n",
    "                AWRe = [np.divide( (sum([((2* (np.sqrt(x[0])/np.pi)/x[1])*(x[0])) for x in Re_param])), sum([x[0] for x in Re_param]) ) ]     \n",
    "                \n",
    "                width_list = []\n",
    "                for idxw, length in lines_index_lst_width:\n",
    "                    idx, idy = [x[0] for x in idxw],  [x[1] for x in idxw]\n",
    "                    width_seg = np.mean(euc_dist_trans_array[idx, idy])\n",
    "                    width_list.append((width_seg*2, length))\n",
    "                                \n",
    "                try:\n",
    "                    AWMPW = np.average([w for w,l in width_list], weights = [a for a,l in Re_param])\n",
    "                except:\n",
    "                    AWMPW = 0\n",
    "        \n",
    "                MPW = np.average([w for w,l in width_list])\n",
    "\n",
    "                try:                   \n",
    "                    AWMPL = np.average([l for w,l in width_list], weights = [a for a,l in Re_param])\n",
    "                except:\n",
    "                    AWMPL = 0\n",
    "\n",
    "                #Append results\n",
    "                date_list.append((pd.to_datetime(str(bbox_da_mask[num].time.values)).strftime('%Y-%m-%d')))\n",
    "                length_list.append((sum((_dict_wet_prop[d]['length']) for d in _dict_wet_prop)))\n",
    "                total_wet_area_list.append(total_wet_area[0])\n",
    "                total_wet_perimeter_list.append(total_wet_perimeter[0])\n",
    "                n_pools_list.append(len(_dict_wet_prop))\n",
    "                \n",
    "                AWMSI_arr = np.concatenate((AWMSI_arr, AWMSI), axis=None)\n",
    "                AWRe_arr = np.concatenate((AWRe_arr, AWRe), axis=None)\n",
    "                AWMPS_arr = np.concatenate((AWMPS_arr, AWMPS), axis=None)\n",
    "                AWMPL_arr = np.concatenate((AWMPL_arr, AWMPL), axis=None)\n",
    "                AWMPW_arr = np.concatenate((AWMPW_arr, AWMPW), axis=None)\n",
    "                \n",
    "                MPW_arr = np.concatenate((MPW_arr, MPW), axis=None)\n",
    "            \n",
    "                try:\n",
    "                    da_area = np.concatenate(([da_area, layer_clip[np.newaxis, :, :]]), axis=0)\n",
    "                except:\n",
    "                    da_area = layer_clip[np.newaxis, :, :]\n",
    "                           \n",
    "                layer_clip = np.where(layer_clip == -1, layer_clip, 0)\n",
    "                # layer_clip = layer_clip[np.newaxis, :, :]\n",
    "                \n",
    "                def list_index (layer_clip, index_lst):\n",
    "                    #retrieve index from valid points\n",
    "                    ind_x = []\n",
    "                    ind_y = []\n",
    "                    for indx in index_lst:\n",
    "                        ind_x.append(indx[0])\n",
    "                        ind_y.append(indx[1])\n",
    "                    layer_clip[ind_x, ind_y] = 1\n",
    "                    layer_zeros = layer_clip[np.newaxis, :, :]\n",
    "                    return layer_zeros\n",
    "                \n",
    "                layer_npools = list_index (layer_clip, n_pools_index_lst)\n",
    "                try:\n",
    "                    da_npools = np.concatenate(([da_npools, layer_npools]), axis=0)\n",
    "                except:\n",
    "                    da_npools = layer_npools\n",
    "\n",
    "                layer_rlines = list_index (layer_clip, lines_index_lst)\n",
    "                try:\n",
    "                    da_rlines = np.concatenate(([da_rlines, layer_rlines]), axis=0)\n",
    "                except:\n",
    "                    da_rlines = layer_rlines                \n",
    "                \n",
    "                if self.export_shp == True:\n",
    "                    try:\n",
    "                        self.save_shp (_dict_wet_prop)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                t_end = time.perf_counter()\n",
    "                clear_output()\n",
    "                #estimate time\n",
    "                total_iterations = estimate_time (total_iterations, t_begin, t_end, est_time_lst)\n",
    "                \n",
    "            clear_output()\n",
    "\n",
    "            print('Exporting results...')           \n",
    "            \n",
    "            pand_dict= {'date': date_list, 'sec_area': seg_area, 'wet_area_km2': [d/10**6 for d in total_wet_area_list], \n",
    "                        'wet_perimeter_km2': [d/10**6 for d in total_wet_perimeter_list], 'wet_length_km': [d/1000 for d in length_list], \n",
    "                        'npools': n_pools_list,  'AWMSI':AWMSI_arr.tolist(), 'AWRe': AWRe_arr.tolist(), \n",
    "                        'AWMPS': [d/10**6 for d in AWMPS_arr.tolist()], 'AWMPL': [d/1000 for d in AWMPL_arr.tolist()], \n",
    "                        'AWMPW': [d*10/1000 for d in AWMPW_arr.tolist()], 'MPW': [d*10/1000 for d in MPW_arr.tolist()]}        \n",
    "            \n",
    "            self.pd_metrics = pd.DataFrame(data=pand_dict)\n",
    "            \n",
    "            da_area = create_dataarray (_layer, da_area, date_list)\n",
    "            export_netcdf (self.outdir, da_area, 'da_area.nc')\n",
    "            \n",
    "            da_npools = create_dataarray (_layer, da_npools, date_list)\n",
    "            export_netcdf (self.outdir, da_npools, 'da_npools.nc')\n",
    "\n",
    "            da_rlines = create_dataarray (_layer, da_rlines, date_list)\n",
    "            export_netcdf (self.outdir, da_rlines, 'da_rlines.nc')\n",
    "            \n",
    "            \n",
    "            create_new_dir(self.outdir)\n",
    "            create_new_dir(os.path.join(self.outdir, 'raster'))\n",
    "            self.outdir_raster = os.path.join(self.outdir, 'raster')\n",
    "            \n",
    "            pdm = self.ini_metrics ()\n",
    "            pdm = self.pixel_level (da_area, pdm)\n",
    "            pdm.to_csv(os.path.join(self.outdir, 'pd_metrics.csv'))\n",
    "           \n",
    "            clear_output()\n",
    "            \n",
    "        result = pd.DataFrame()\n",
    "        for i in range(0, len(rcor_extent)):\n",
    "            outdir = os.path.join(first_path, f'{i}', 'pd_metrics.csv')\n",
    "            df = pd.read_csv(outdir, index_col=0)\n",
    "            df['Section'] = i + 1\n",
    "            result = pd.concat([result, df], axis=0, ignore_index=True)\n",
    "\n",
    "        result.to_csv(os.path.join(first_path, 'merged_segs.csv'))\n",
    "        \n",
    "        print('All done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiverConnect ():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def water_detect_batch(self, input_img, ini_file, rcor_extent, buffer = 1000, img_ext = '.tif', reg = None, max_cluster = None, export_tif = False):\n",
    "        \n",
    "        da_wmask, outdir = WaterDetectBatch (input_img, ini_file, rcor_extent, buffer, img_ext, reg, max_cluster, export_tif).wd_batch()\n",
    "\n",
    "    def metrics(self, da_wmask, rcor_extent, export_shp = False, export_raster = True, outdir=None, sec_length= None):\n",
    "        \n",
    "        # da_area, da_npools, da_rlines, pd_metrics = Metrics (da_wmask, rcor_extent, export_shp, outdir).run()\n",
    "        \n",
    "        Metrics (da_wmask, rcor_extent, export_shp, export_raster, outdir, sec_length).run()     \n",
    "        \n",
    "        \n",
    "    \n",
    "#     def patterns(self, da_area=None, da_npools=None, pd_metrics=None, export_raster = False, outdir=None, sec_length= None):\n",
    "\n",
    "#         pdm = Patterns(da_area, da_npools, pd_metrics, export_raster, outdir, sec_length).run()\n",
    "         \n",
    "            \n",
    "    def run(self, input_img, ini_file, rcor_extent, buffer = 1000, img_ext = '.tif', reg = None, max_cluster = None, export_tif = False,\n",
    "            export_shp = False, export_raster = False, outdir=None, sec_length= None):\n",
    "        \n",
    "        da_wmask, outdir = WaterDetectBatch (input_img, ini_file, rcor_extent, buffer, img_ext, reg, max_cluster, export_tif).wd_batch()\n",
    "        \n",
    "        da_area, da_npools, da_rlines, pd_metrics = Metrics (da_wmask, rcor_extent, export_shp, outdir).run()\n",
    "               \n",
    "        pdm = Patterns(da_area, da_npools, pd_metrics, export_raster, outdir, sec_length).run()\n",
    "        \n",
    "        \n",
    "        # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = RiverConnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcor_extent = r'D:\\Chapter_2_2\\Metrics_visual_analysis\\shp\\sva_buff_sort.shp'\n",
    "da_wmask = r'D:\\Chapter_2_2\\Metrics_visual_analysis\\unseg6_del\\results_RiverConnect\\wd_batch\\netCDF\\s2_wd_merge.nc'\n",
    "sec_len = 10.1743\n",
    "\n",
    "x.metrics(da_wmask, rcor_extent, sec_length=sec_len, export_shp = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n",
      "Wall time: 3min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Full Catchment - 4 parts\n",
    "\n",
    "\n",
    "### CURRENT CHAP 5\n",
    "\n",
    "path = r'D:\\Chapter_4\\segments'\n",
    "\n",
    "# path_list = [\n",
    "#              # ('seg5','fr_main_channel_5.shp', 67.828), ('seg15','fr_main_channel_15.shp', 22.609), ('seg25','fr_main_channel_25.shp', 13.566), \n",
    "#              # ('seg50','fr_main_channel_50.shp', 6.783), ('seg100','fr_main_channel_100.shp', 3.391), ('seg200','fr_main_channel_200.shp', 1.696), \n",
    "#              # ('seg300','fr_main_channel_300.shp', 1.13), \n",
    "#              # ('seg400','fr_main_channel_400.shp', 0.848), ('seg500','fr_main_channel_500.shp', 0.678), \n",
    "#              ('seg600','fr_main_channel_600.shp', 0.565), ('seg700','fr_main_channel_700.shp', 0.484)]\n",
    "\n",
    "\n",
    "# path_list = [('seg500','fr_main_channel_500.shp', 0.678), \n",
    "#              ('seg600','fr_main_channel_600.shp', 0.565), ('seg700','fr_main_channel_700.shp', 0.484)]\n",
    "\n",
    "path_list = [#('seg1','sampled_pools_polygons.shp', 0.484),\n",
    "             # ('seg2_edited','sampled_pools_polygons2.shp', 0.484),\n",
    "             ('seg3_edited','sampled_pools_polygons3.shp', 0.484)]\n",
    "\n",
    "nc_path = r'results_RiverConnect\\wd_batch\\netCDF\\s2_wd_merge.nc'\n",
    "\n",
    "for seg, shp, slen in path_list:\n",
    "    da_wmask = os.path.join(path, seg, nc_path)\n",
    "    rcor_extent = os.path.join(path, 'shp', shp)\n",
    "    sec_len = slen\n",
    "    x.metrics(da_wmask, rcor_extent, sec_length=sec_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
